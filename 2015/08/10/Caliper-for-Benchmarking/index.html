<!DOCTYPE html>
<html>

<head>
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" >
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>
  <title>Caliper for Benchmarking | Open-Estuary</title>
  <meta name="description" content="Open-Estuary Community Boards Support Site" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="MobileOptimized" content="320" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="stylesheet" type="text/css" href="/css/component.css" />
  <link rel="stylesheet" type="text/css" href="/css/screen.css" />
  <link rel="stylesheet" type="text/css" href="/css/jq-metro.css" />
  <meta name="generator" content="Open-Estuary">
  <script src="http://static.duoshuo.com/embed.js"></script>
  
  
  

  
</head>
<body>
<div class="container">
    <div class="mp-pusher" id="mp-pusher">
        <i id="scroll-up" class="fa fa-angle-up"></i>
        <nav id="mp-menu" class="mp-menu">
            <div class="mp-level">
                <a data-pjax class="back-home" style="font-size: 20px" href="/"><h2 ><i class="fa fa-home"></i>
                        Home</h2></a>
                <ul class="first-level">
                    <li>
                        <a class="fa fa-archive" href="#"><i class="fa fa-angle-left">
                            </i>&nbsp;&nbsp;Archive</a>
                        <div class="mp-level page-list">
                            <h2 ><i class="fa fa-archive"></i>
                                Archive</h2>
                            <a class="mp-back" href="#">back</a>
                            <form id="search-form" action="">
                                <input type="text" class="search search-archive" placeholder="Search.."/>
                            </form>
                            <ul>
                                <div class="mp-scroll">
                                
                                <li class="search-archive-li mp-pjax">
                                    <a href="/2015/08/10/Overview-Introduction/">Overview Introduction</a>
                                </li>
                                
                                <li class="search-archive-li mp-pjax">
                                    <a href="/2015/08/10/Quick-Start/">Estuary QuickStart Guide</a>
                                </li>
                                
                                <li class="search-archive-li mp-pjax">
                                    <a href="/2015/08/10/Go-Through/">Go Through</a>
                                </li>
                                
                                <li class="search-archive-li mp-pjax">
                                    <a href="/2015/08/10/Binary-Files/">Binary Files</a>
                                </li>
                                
                                <li class="search-archive-li mp-pjax">
                                    <a href="/2015/08/10/Shared-Boards-in-Open-Lab/">Shared Boards in Open Lab</a>
                                </li>
                                
                                <li class="search-archive-li mp-pjax">
                                    <a href="/2015/08/10/Caliper-for-Benchmarking/">Caliper for Benchmarking</a>
                                </li>
                                
                                <li class="search-archive-li mp-pjax">
                                    <a href="/2015/08/10/UEFI-and-Grub/">UEFI and Grub</a>
                                </li>
                                
                                <li class="search-archive-li mp-pjax">
                                    <a href="/2015/08/10/Hardware-Boards/">Hardware Boards</a>
                                </li>
                                
                                <li class="search-archive-li mp-pjax">
                                    <a href="/2015/08/10/Armor-Tools/">Armor Tools</a>
                                </li>
                                
                                <li class="search-archive-li mp-pjax">
                                    <a href="/2015/04/17/D06/">D06-hacking-manual</a>
                                </li>
                                
                                <li class="search-archive-li mp-pjax">
                                    <a href="/2014/12/23/D05/">D05-hacking-manual</a>
                                </li>
                                
                                </div>
                            </ul>
                        </div>
                    </li>
                    <li>
                        <a class="fa fa-copy" href="#"><i class="fa fa-angle-left">
                            </i>&nbsp;&nbsp;Categories</a>

                        <div class="mp-level page-list">
                            <h2 ><i class="fa fa-copy"></i>
                                Categories</h2>
                            <a class="mp-back" href="#">back</a>
                            <form id="search-form" action="">
                                <input type="text" class="search search-category" placeholder="Search.."/>
                            </form>
                            <ul>
                                <div class="mp-scroll">
                                
                                </div>
                            </ul>
                        </div>
                    </li>
                    <li>
                        <a class="fa fa-tags" href="#"><i class="fa fa-angle-left">
                            </i>&nbsp;&nbsp;Tags</a>
                        <div class="mp-level page-list">
                            <h2 ><i class="fa fa-tags"></i>
                                Tags</h2>
                            <a class="mp-back" href="#">back</a>
                            <form id="search-form" action="">
                                <input type="text" class="search search-tag" placeholder="Search.."/>
                            </form>
                            <ul>
                                <div class="mp-scroll">
                                
                                <li class="search-tag-li mp-pjax">
                                    <a href="/tags/Armor-Tools/">&nbsp;&nbsp;&nbsp;Armor Tools</a>
                                    <small>1</small>
                                </li>
                                
                                <li class="search-tag-li mp-pjax">
                                    <a href="/tags/Binary-Files/">&nbsp;&nbsp;&nbsp;Binary Files</a>
                                    <small>1</small>
                                </li>
                                
                                <li class="search-tag-li mp-pjax">
                                    <a href="/tags/Caliper-for-Benchmarking/">&nbsp;&nbsp;&nbsp;Caliper for Benchmarking</a>
                                    <small>1</small>
                                </li>
                                
                                <li class="search-tag-li mp-pjax">
                                    <a href="/tags/D05/">&nbsp;&nbsp;&nbsp;D05</a>
                                    <small>1</small>
                                </li>
                                
                                <li class="search-tag-li mp-pjax">
                                    <a href="/tags/D06/">&nbsp;&nbsp;&nbsp;D06</a>
                                    <small>1</small>
                                </li>
                                
                                <li class="search-tag-li mp-pjax">
                                    <a href="/tags/Go-Through/">&nbsp;&nbsp;&nbsp;Go Through</a>
                                    <small>1</small>
                                </li>
                                
                                <li class="search-tag-li mp-pjax">
                                    <a href="/tags/Hardware-Boards/">&nbsp;&nbsp;&nbsp;Hardware Boards</a>
                                    <small>1</small>
                                </li>
                                
                                <li class="search-tag-li mp-pjax">
                                    <a href="/tags/Overview-Introduction/">&nbsp;&nbsp;&nbsp;Overview Introduction</a>
                                    <small>1</small>
                                </li>
                                
                                <li class="search-tag-li mp-pjax">
                                    <a href="/tags/Quick-Start/">&nbsp;&nbsp;&nbsp;Quick Start</a>
                                    <small>1</small>
                                </li>
                                
                                <li class="search-tag-li mp-pjax">
                                    <a href="/tags/Shared-Boards-in-Open-Lab/">&nbsp;&nbsp;&nbsp;Shared Boards in Open Lab</a>
                                    <small>1</small>
                                </li>
                                
                                <li class="search-tag-li mp-pjax">
                                    <a href="/tags/UEFI-and-Grub/">&nbsp;&nbsp;&nbsp;UEFI and Grub</a>
                                    <small>1</small>
                                </li>
                                
                                </div>
                            </ul>
                        </div>
                    </li>
                    
                    <li><a class="fa fa-user" href="http://www.open-estuary.io">&nbsp;&nbsp;&nbsp;About me</a></li>
                    <li><a class="fa fa-github" href="https://github.com/open-estuary">&nbsp;&nbsp;&nbsp;Github</a></li>
                    <li><a class="fa fa-bullhorn" href="http://open-estuary.com">&nbsp;&nbsp;&nbsp;New Site</a></li>
                </ul>

            </div>
        </nav>
        <div id="pjax">
            <div class="pjax-hidden" style="display: none">
                
                    <a  data-pjax href="/2015/08/10/Overview-Introduction/">Overview Introduction</a>
                
                    <a  data-pjax href="/2015/08/10/Quick-Start/">Estuary QuickStart Guide</a>
                
                    <a  data-pjax href="/2015/08/10/Go-Through/">Go Through</a>
                
                    <a  data-pjax href="/2015/08/10/Binary-Files/">Binary Files</a>
                
                    <a  data-pjax href="/2015/08/10/Shared-Boards-in-Open-Lab/">Shared Boards in Open Lab</a>
                
                    <a  data-pjax href="/2015/08/10/Caliper-for-Benchmarking/">Caliper for Benchmarking</a>
                
                    <a  data-pjax href="/2015/08/10/UEFI-and-Grub/">UEFI and Grub</a>
                
                    <a  data-pjax href="/2015/08/10/Hardware-Boards/">Hardware Boards</a>
                
                    <a  data-pjax href="/2015/08/10/Armor-Tools/">Armor Tools</a>
                
                    <a  data-pjax href="/2015/04/17/D06/">D06-hacking-manual</a>
                
                    <a  data-pjax href="/2014/12/23/D05/">D05-hacking-manual</a>
                
                
                
                    <a data-pjax href="/tags/Armor-Tools/">&nbsp;&nbsp;Armor Tools</a>
                
                    <a data-pjax href="/tags/Binary-Files/">&nbsp;&nbsp;Binary Files</a>
                
                    <a data-pjax href="/tags/Caliper-for-Benchmarking/">&nbsp;&nbsp;Caliper for Benchmarking</a>
                
                    <a data-pjax href="/tags/D05/">&nbsp;&nbsp;D05</a>
                
                    <a data-pjax href="/tags/D06/">&nbsp;&nbsp;D06</a>
                
                    <a data-pjax href="/tags/Go-Through/">&nbsp;&nbsp;Go Through</a>
                
                    <a data-pjax href="/tags/Hardware-Boards/">&nbsp;&nbsp;Hardware Boards</a>
                
                    <a data-pjax href="/tags/Overview-Introduction/">&nbsp;&nbsp;Overview Introduction</a>
                
                    <a data-pjax href="/tags/Quick-Start/">&nbsp;&nbsp;Quick Start</a>
                
                    <a data-pjax href="/tags/Shared-Boards-in-Open-Lab/">&nbsp;&nbsp;Shared Boards in Open Lab</a>
                
                    <a data-pjax href="/tags/UEFI-and-Grub/">&nbsp;&nbsp;UEFI and Grub</a>
                
                <a data-pjax class="fa fa-user" href="/about">&nbsp;&nbsp;&nbsp;About me</a>
            </div>
            <nav class="nexus">
                <li  style="border-left: 1px solid #c6d0da;">
                    <a id="trigger" href="#"><i class="fa fa-bars"></i></a>
                </li>
                <li ><a id="nexus-back" href="/">Open-Estuary</a></li>
                
                <div id="nav-container">
                    <div class="post-navbar" style="line-height: 63px;display:none">
                        <li id="navbar-title"><a href="#">Caliper for Benchmarking</a></li>
                        <li id="navbar-toc" style="border-left: none">
                            <a style="padding-right: 15px">
                                <span id="toc-content" >Introduction</span><i class="fa fa-chevron-down" ></i>
                            </a>
                            <div class="hidden-box">
                                <ul id="toc"></ul>
                            </div>
                        </li>
                    </div>
                </div>
                
            </nav>

            <div class="scroller">
            <div class="scroller-inner">


<!-- -->
<!--<body class="post-template">-->
<!---->
  

<main class="content" role="main">
    <article class="post" >
    <span class="post-meta">
                  <div class="tag-tile">
                      
                      
                      <a data-pjax href='/tags/Caliper-for-Benchmarking/' style='color:#D5D5D5'>Caliper for Benchmarking</a>
                      
                      
                  </div>
                <h1 class="post-title" style="margin: 14px 0;color:#50585D">Caliper for Benchmarking</h1>

                    <div class="post-meta">
                        Post on<span class="fa fa-clock-o"></span>
                        <time datetime="2015-08-10T02:14:21.000Z"
                              itemprop="datePublished">Aug 10 2015</time>
                    </div>
    </span>

        <section class="post-content">
            <h2 id="Welcome_to_the_Caliper_wiki!">Welcome to the Caliper wiki!</h2>
<h4 id="What_is_Caliper?">What is Caliper?</h4>
<p>Caliper is a test suite focused on functional integrity and performance evaluation of boards, it not only detects if the hardware and software of the board can work well, but also tests the performance. Caliper can be run on boards to get test data, then converse the test data to scores by a series of formulas. Then the test data can be easily to read. Thereby caliper intuitively presents various performance values and shows the performance gaps of the boards. Caliper’s ultimate goals: one is that it can generate a series of test data which provides inputs for specific boards optimization; the other is, to help to analyze the functional stability and performance of software solution (especially Estuary and Harmonic) on hardware platforms.</p>
<h4 id="How_Caliper_works?">How Caliper works?</h4>
<p>Caliper mainly builds on the existing open source benchmarks and test suites. It can be divided into the following parts: functional testing and performance testing, each of them is known as a “System Test Item”.  Caliper’s functional testing mainly use the LTP (Linux Test Project), which contains a lot of tests focused on systems’ functions and features.  Caliper’s performance assessment includes the following aspects: real-time, CPU, memory, network, file system and so on; any aspect referred above is called “Test Sub-Items” herein. When Caliper test a Test Sub-Items, it needs to test many meaningful evaluable items which constitute the Test Sub-Item, this kind of evaluable items are called “Test Points” (such as the delay of the network, and so on). Small granularity division of a Test Point is a “Test Case”, which is the minimum unit. </p>
<h2 id="How_to_Use_Caliper?">How to Use Caliper?</h2>
<p>The test suite mainly includes performance test cases, it can be used to test the performance of machine, and now we have not integrated many functional tests. The test suite can run on Linux, the machines can belong to x86_64, arm_32, arm_64. Here is steps to setup testbed.</p>
<h3 id="Work_Mode">Work Mode</h3>
<ol>
<li>Host and target<br>The host and the target all run Linux. The Host can access the Target with SSH, you should better scp the public key of Host to the Linux target so that the host can access the target without password. Also you need to install the compiling chain in the host for the Linux target.</li>
</ol>
<h3 id="Environment_Installation">Environment Installation</h3>
<h5 id="Requirements">Requirements</h5>
<pre><code>Python <span class="number">2.7</span>
Linux
SSH <span class="keyword">for</span> Linux devices
Compilation Chain <span class="keyword">for</span> Linux devices
</code></pre><h5 id="Host_OS_installation">Host OS installation</h5>
<p>It supports x86_64 CentOS6, OpenSUSE and Ubuntu platform, you need install 64bit CentOS system or Ubuntu system on your PC or server platform.</p>
<h5 id="Toolchain_installation">Toolchain installation</h5>
<p>To build arm/android target binary, it requires arm/android toolchain deployment. We can download the existing compiled toolchains from some website.<br>Here is website to download ARM toolchains:<br><a href="https://releases.linaro.org/13.10/components/toolchain/binaries/gcc-linaro-arm-linux-gnueabihf-4.8-2013.10_linux.tar.bz2" target="_blank" rel="external">https://releases.linaro.org/13.10/components/toolchain/binaries/gcc-linaro-arm-linux-gnueabihf-4.8-2013.10_linux.tar.bz2</a>,  which is for the target that is arm_32.<br><a href="https://releases.linaro.org/13.10/components/toolchain/binaries/gcc-linaro-aarch64-linux-gnu-4.8-2013.10_linux.tar.bz2" target="_blank" rel="external">https://releases.linaro.org/13.10/components/toolchain/binaries/gcc-linaro-aarch64-linux-gnu-4.8-2013.10_linux.tar.bz2</a>, which is for the target that is arm_64.</p>
<p>For ubuntu, you can directly use the following commands to install the toolchains.</p>
<pre><code>sudo apt-get <span class="operator"><span class="keyword">install</span> gcc-aarch64-linux-gnu -y
sudo apt-<span class="keyword">get</span> <span class="keyword">install</span> gcc-arm-linux-guneabihf -y</span>
</code></pre><p>Note: Current building for x86_32 platform is not supported now.  For cross compiler, the path of tool-chain need to be added in the $PATH in the Host.*</p>
<h3 id="Download_and_Install">Download and Install</h3>
<pre><code>git <span class="keyword">clone</span> http:<span class="comment">//github.com/HTSAT/caliper.git</span>
</code></pre><p>This will download a directory named caliper in your current operation directory.</p>
<p>Enter the test suite<br>    <code>cd caliper</code><br>Install Caliper (Optional)<br>    <code>sudo python setup.py install</code><br>    When you install Caliper in your system, the config files locate in the <code>/etc/caliper/config/</code> and <code>/etc/caliper/test_cases_cfg/</code>.</p>
<p>Not Install Caliper (running Caliper in your home directory).<br>    The config file locate in the <code>config</code> and <code>test_cases_cfg</code> directories under the Caliper root directory.</p>
<h3 id="Configure_and_Run_Caliper">Configure and Run Caliper</h3>
<h5 id="Configure_Caliper">Configure Caliper</h5>
<p>1.Configure the target<br>Configure the <code>config/client_config.cfg</code> file to set up the target.</p>
<p>2.Configure the mail list<br>Configure the <code>config/email_config.cfg</code> file to determine who will send and receive the mails, the mail contents will be the test reports.</p>
<p>3.Configure the execution way<br>This means when the processes of building and running occurred error, if caliper will be stopped.</p>
<p>4.Select the benchmarks you want to run<br>Configure the config files located in <code>test_cases_cfg/XXXX_cases_cfg.cfg</code>(XXXX can be <code>android</code>, <code>server</code>, <code>arm</code> and <code>common</code>) to select the benchmarks you want to run.</p>
<p>5.Configure the test cases you want to run in a benchmark<br>Configure the <code>test_cases_cfg/XXXX/benchmark_name/benchmark_name_run.cfg</code>(XXXX can be <code>android</code>, <code>server</code>, <code>arm</code> and <code>common</code>) files to select the test cases you want to run.</p>
<h5 id="Run_Caliper">Run Caliper</h5>
<p>If you have configured your environment, you can enter the commands of <code>caliper</code> to run caliper, it will compile and execute test cases, parser the output and get the summarization of the outputs.<br>The command is <code>caliper</code>. You can use <code>caliper -h</code> to show all the commands options. After the process finished, you can view the generated files, including the log files, binary files, test results and so on. The logs and the test results will locate in the <code>~/.caliper</code> if you have installed caliper. Otherwise, they will locate in the Caliper root directory when you run caliper from the Caliper source code.</p>
<h5 id="Caliper_output">Caliper output</h5>
<p>After the <code>caliper -option</code> command has been finished, the results generated are<br>in the <code>results</code> folder, the <code>results</code> folder contains all the results. The logs<br>will be in <code>caliper_build</code> and <code>caliper_exec</code> folder. Each benchmarks has two<br>related log files about the execution and parsering, named <code>XXX_output.log</code> and<br><code>XXX_parser.log</code>. In addtion, all result of selected benchmarks’ test points are<br>stored in the yaml file, which located in <code>results/yaml</code>.</p>
<p>You can see the comparision figures in the <code>results/test_results.tar.gz</code>, the tarball is a webpage tarball. If it is not there, maybe you have a wrong when generating the webpages or you have not select the option to generate the webpage. Or you can see the specific values of Test Cases and scores in <code>results/yaml/your_machine_name.yaml</code>,<br><code>results/yaml/your_machine_name_score.yaml</code> and<br><code>results/yaml/your_machine_name_score_post.yaml</code>.</p>
<h6 id="The_Format_of_Yaml">The Format of Yaml</h6>
<pre><code>name: your_machine_name
<span class="attribute">...</span><span class="attribute">...</span>
results:
  Performance:
    latency:
      process:
        Point_Scores:
          lat_proc_exec: <span class="number">5.77</span>
          lat_proc_fork: <span class="number">5.95</span>
          lat_proc_shell: <span class="number">2.34</span>
          lat_sig_catch: <span class="number">2.86</span>
          lat_sig_install: <span class="number">4.69</span>
          <span class="attribute">...</span><span class="attribute">...</span>
    network:
      latency:
        Point_Scores:
          lat_connect: <span class="number">0</span>
          lat_pipe: <span class="number">2.97</span>
          <span class="attribute">...</span><span class="attribute">...</span>
</code></pre><p>The <code>Performance</code> means a Test Item; the <code>latency</code>, <code>memory</code> and <code>network</code> are Test Sub-Items which belong to the <code>Performance</code>; In the Test Sub-Item of <code>memory</code>, the <code>bandwidth</code> is a Test Point; in <code>bandwidth</code>, some Test Cases has been tested, so in the <code>Point Scores</code>  has some key-value pairs, such as <code>bw_mem_bzero: 246.7238</code>.<br><em>Note: in some key-value pairs, the value of ‘0’ means that the test case is failed.</em></p>
<h2 id="Architecture_&amp;_Contribute_more_benchmarks">Architecture &amp; Contribute more benchmarks</h2>
<h4 id="The_architecture_of_Caliper">The architecture of Caliper</h4>
<p>There are several files and folders in the test suite, they are listed in the follow.<br><code>benchmarks  client     common.pyc  frontend     README.md  setup.py
caliper     common.py  config      __init__.py  server     test_cases_cfg</code></p>
<p><strong>benchmarks</strong>: store the benchmarks, they can be downloaded or written by yourself.</p>
<p><strong>caliper</strong>: run <code>./caliper</code>, the benchmarks which was configured in the <code>test_cases_cfg/XXXX_cases_def.cfg</code>(XXXX can be <code>common</code> and some like that) will be compiled and generate the executable files in the build directory. Then the Host will scp the generated execution files to the Target, and then control the Target to run the commands which has been configured in the <code>test_cases_cfg/XXXX/benchmark_name/benchmark_name_run.cfg</code> and get the results of the command, then the host will parser the outputs. Caliper will run the commands and parser the commands one by one </p>
<p><strong>server</strong>: This directory contains the scripts for dispatching the build, run and parser on the Host and remote login in the Target. Also part of scripts in <code>server</code> directory will use the function in the directory named of <code>client</code>. The thought of <code>client</code> and <code>server</code> is borrowed from the Autotest.</p>
<pre><code>.
├── build
├── common.<span class="keyword">py</span>
├── compute_model
├── hosts
├── __init__.<span class="keyword">py</span>
├── parser_process
├── run
├── test_host.<span class="keyword">py</span>
└── utils.<span class="keyword">py</span>
</code></pre><p>The <code>build</code> directory is for building the benchmarks in Caliper.<br>The <code>compute_model</code> directory mainly include the method to get the score from the parser output, the method of scoring mainly in the scores_method.py.<br>The <code>parser_process</code> is mainly about the process of parsering the output of benchmarks, traverse the output to the score and draw the diagrams for Caliper.<br>The <code>run</code> directory mainly includes the test_run.py, it is the main code to execute the commands in benchmarks and the parser function defined by each benchmarks.<br>The <code>hosts</code> directory mainly contains the class of hosts and how to use hosts.</p>
<p><strong>test_cases_cfg</strong>: benchmarks which will be compiled and run are defined in this directory. </p>
<h4 id="Add_benchmarks_to_Caliper">Add benchmarks to Caliper</h4>
<p>If a benchmark need to be added in Caliper, some steps should be done.</p>
<h5 id="1-the_structure_of_test_cases_definition">1.the structure of test cases definition</h5>
<p>The directory named of <code>test_cases_def</code> is the key of how to build, run and parser. The tree of it is listed in the follow. If you want to add a benchmark, you not only need to add the section about it in XXX_cases_def.cfg (XXX can be common, server arm, or android, this depends on your classfication of the test cases), but also need to define the build process of build, the config file of run, and the parser scripts used to get results.</p>
<pre><code>.
├── android
├── android_cases_def.cfg
├── arm
├── arm_cases_def.cfg
├── <span class="keyword">common</span>
├── common_cases_def.cfg
├── README
├── <span class="keyword">server</span>
└── server_cases_def.cfg
</code></pre><p>The architecture of <code>common</code> directory looks like below. Namely, the build script, the run config file should be added in the directory. If the test cases need server and client, then you need to have a more <code>XXX_server_run.cfg</code>, it is used to run the server’s commands. In addition, the parser file of a benchmark, which named <code>iperf_parser.py</code> or something like that, should be added in the <code>client/parser/</code>.</p>
<pre><code>.
├── iperf
│   ├── iperf_build.<span class="keyword">sh</span>
│   ├── iperf_run.cfg
│   └── iperf_server_run.cfg
└── rttest
    ├── rttest_build.<span class="keyword">sh</span>
    └── rttest_run.cfg
</code></pre><h5 id="2-_Define_the_benchmark">2. Define the benchmark</h5>
<p>Add the corresponding information in <strong>test_cases_cfg/test_cases_define.cfg</strong>. The format of the info is listed below.</p>
<pre><code><span class="title">[lmbench]</span>
<span class="setting">build = <span class="value">lmbench_build.sh</span></span>
<span class="setting">run = <span class="value">lmbench_run.cfg</span></span>
<span class="setting">parser = <span class="value">lmbench_parser.py</span></span>
</code></pre><p>The options of <code>build</code>, <code>run</code> and <code>parser</code> are indispensable. The values in the section are all files which need to be located in the classfication folder(common, arm, server and so on).</p>
<h5 id="3-_‘Build’_the_benchmark">3. ‘Build’ the benchmark</h5>
<p>The script file which is specified by the <code>build</code> option can compile the benchmark. The exsiting shell script of other benchmarks can be referenced. The path should be taken into consideration. Take the scimark build for example.</p>
<p>the scimark build scripts:</p>
<pre><code><span class="number">1</span> <span class="function"><span class="title">build_scimark</span></span>() {
<span class="number">2</span>     <span class="keyword">set</span> <span class="operator">-e</span>
<span class="number">3</span>     SrcPath=<span class="variable">${BENCH_PATH}</span><span class="string">"402.scimark"</span>
<span class="number">4</span>     myOBJPATH=<span class="variable">${INSTALL_DIR}</span>/bin
<span class="number">5</span>     <span class="built_in">pushd</span> <span class="variable">$SrcPath</span>
<span class="number">6</span>     <span class="keyword">if</span> [ <span class="variable">$ARCH</span> = <span class="string">"x86_32"</span> -o <span class="variable">$ARCH</span> = <span class="string">"x86_64"</span> ]; <span class="keyword">then</span>
<span class="number">7</span>         make CC=<span class="variable">$GCC</span> CFLAGS=<span class="string">"-msse4"</span>
<span class="number">8</span>         cp scimark2 <span class="variable">$myOBJPATH</span>/
<span class="number">9</span>         make CC=<span class="variable">$GCC</span> clean
<span class="number">10</span>     <span class="keyword">fi</span>
<span class="number">11</span>     <span class="keyword">if</span> [ <span class="variable">$ARCH</span> = <span class="string">"arm_32"</span> ]; <span class="keyword">then</span>
<span class="number">12</span>         make CC=<span class="variable">$GCC</span> CFLAGS=<span class="string">" -mfloat-abi=hard -mfpu=vfpv4 -mcpu=cortex-a15 "</span>
<span class="number">13</span>         cp scimark2 <span class="variable">$myOBJPATH</span>/
<span class="number">14</span>         make CC=<span class="variable">$GCC</span> clean
<span class="number">15</span>     <span class="keyword">fi</span>
<span class="number">16</span>     <span class="keyword">if</span> [ <span class="variable">$ARCH</span> = <span class="string">"arm_64"</span> ]; <span class="keyword">then</span>
<span class="number">17</span>         make CC=<span class="variable">$GCC</span>
<span class="number">18</span>         cp scimark2 <span class="variable">$myOBJPATH</span>
<span class="number">19</span>         make CC=<span class="variable">$GCC</span> clean
<span class="number">20</span>     <span class="keyword">fi</span>
<span class="number">21</span> 
<span class="number">22</span>     <span class="keyword">if</span> [ <span class="variable">$ARCH</span> = <span class="string">"android"</span> ]; <span class="keyword">then</span>
<span class="number">23</span>         ndk-build
<span class="number">24</span>         cp libs/armeabi-v7a/scimark2 <span class="variable">$myOBJPATH</span>/
<span class="number">25</span>         ndk-build clean
<span class="number">26</span>         rm -rf libs/ obj/
<span class="number">27</span>     <span class="keyword">fi</span>
<span class="number">28</span>     <span class="built_in">popd</span>
<span class="number">29</span> }
<span class="number">30</span> 
<span class="number">31</span> build_scimark
</code></pre><p>You should change the value of SrcPath and myOBJPATH, to use your benchmaarks name to replace <code>402.scimark</code>and use your <strong>expected name</strong> to resplace <code>bin</code>.<br>Then you can define the build commands in the later space. For different arch, you can use different commands.</p>
<h5 id="4-_‘Run’_the_benchmark">4. ‘Run’ the benchmark</h5>
<p>When you run caliper, if the build process finished, caliper will scp the binary files to the remote target, and then run the commans you defined in the XXX_run.cfg on the remote target. The <code>run</code> option illustrates the configuration of running the benchmark. The content of the configuration file is like this:</p>
<pre><code><span class="number">1</span> [scimark]
<span class="number">2</span> <span class="variable">category =</span> Performance cpu multicore_float scimark
<span class="number">3</span> <span class="variable">scores_way =</span>  compute_speed_score <span class="number">1</span>
<span class="number">4</span> <span class="variable">command =</span> ./bin/scimark2
<span class="number">5</span> <span class="variable">parser =</span> scimark_parser
</code></pre><p>Each section in the configuration is a Test Case. The <code>category</code> key set the value of the Test Case<code>s category. The</code>scores_way<code>is set to compute the score of the Test Case. The method set in the</code>scores_way<code>can be found in</code>scores_method.py<code>in the</code>compute_model<code>directory which locates in</code>server<code>directory. New computation method can be added in that file. The</code>command<code>is the instruction which will be run on the target. The</code>parser<code>set the method to parser the output of the command, the</code>parser<code>must be implemented in the parser file.
Note: the commands in the</code>command<code>must can be found in the binary files</code> directory, it should have the <strong>expected name</strong> in the commands. </p>
<p>Also, we support the different length of <strong>category</strong>. Why we use so many kinds of category, it is because one test case may include many values which belong to different kinds of categories.</p>
<p>1). One</p>
<pre><code><span class="number">1</span> [lmbench]
<span class="number">2</span> category = Performance     (lmbench covers some subsytem, such <span class="keyword">as</span> cpu, network, <span class="operator">and</span> so <span class="command"><span class="keyword">on</span>)</span>
<span class="number">3</span> scores_way = compute_speed_score <span class="number">2</span>
<span class="number">4</span> <span class="command"><span class="keyword">command</span> = <span class="string">'cd lmbench; ./lmbench CONFIG'</span></span>
<span class="number">5</span> parser = lmbench_parser
</code></pre><p>2). Two</p>
<pre><code><span class="number">1</span> [nbench]
<span class="number">2</span> <span class="variable">category =</span> Performance cpu  (nbench covers sincore_int <span class="constant">and</span> sincore_float)
<span class="number">3</span> <span class="variable">scores_way =</span>  compute_speed_score <span class="number">1</span>
<span class="number">4</span> <span class="variable">command =</span> <span class="string">"pushd nbench; ./nbench; popd"</span>
<span class="number">5</span> <span class="variable">parser =</span> nbench_parser
</code></pre><p>3). Three </p>
<pre><code><span class="number">1</span> [iozone]
<span class="number">2</span> category = Performance disk bandwidth  (iozone cover <span class="operator">the</span> <span class="built_in">read</span>, <span class="built_in">write</span> <span class="operator">and</span> so <span class="command"><span class="keyword">on</span> <span class="title">in</span> <span class="title">bandwidth</span>)</span>
<span class="number">3</span> scores_way =  compute_speed_score <span class="number">5</span> 
<span class="number">4</span> <span class="command"><span class="keyword">command</span> = <span class="string">"cd bin; ./iozone -s5g -r1M -I; cd .."</span></span>
<span class="number">5</span> parser = iozone_parser
</code></pre><p>4). Four</p>
<pre><code><span class="number">1</span> [scimark]
<span class="number">2</span> <span class="variable">category =</span> Performance cpu multicore_float scimark
<span class="number">3</span> <span class="variable">scores_way =</span>  compute_speed_score <span class="number">1</span>
<span class="number">4</span> <span class="variable">command =</span> ./bin/scimark2
<span class="number">5</span> <span class="variable">parser =</span> scimark_parser
</code></pre><h5 id="5-_‘Parser’_the_benchmark">5. ‘Parser’ the benchmark</h5>
<p>The parser method has been set, and it must be implemented, in the above example of code, the function of <code>scimark_parser</code> must be in the file of <code>scimark_parser.py</code>. This file should be located in the <code>client/parser</code> folder.</p>
<pre><code><span class="number">8</span> <span class="function"><span class="keyword">def</span> <span class="title">scimark_parser</span><span class="params">(content, outfp)</span>:</span>
<span class="number">9</span>     value = -<span class="number">1</span>
      ...
<span class="number">22</span>    <span class="keyword">return</span> value
</code></pre><p>Notes: the funtion of parser must have two args: the first represents the output of executing commands, it is come from fd.read(); the second is the file pointer which writes the parser log file. In addtion the parser must return an number, which is needed for the later score computing.</p>
<p>Notes: According to the length of the <code>category</code> in XXX_run.cfg, the parser needs to return different values.</p>
<p>1)  Return Three embedded dictionary (the length of category is 1)</p>
<pre><code><span class="number">1</span> [lmbench]
<span class="number">2</span> <span class="variable">category =</span> Performance   
<span class="number">3</span> <span class="variable">scores_way =</span> compute_speed_score <span class="number">2</span>
</code></pre><p>The parser need to return a dictionary.  the CPU part is dic[‘cpu’], memory is dic[‘memory’], etc.  Each element of the list is a dictionary, it looks like this: {multicore_int:{key1:value1, key2: value2 …}, multicore_float:{}}.</p>
<p>But if some values are about latency while others are about bandwidth in the dictionary, it is not scientific to use one formula to get the score for latency and bandwidth, they need different compute methods. The function of <code>compute_speed_score</code> is suitable for the bandwidth and is not suitable for latency.</p>
<p>2) Return Two embedded dictionary  (the length of category is 2)</p>
<pre><code><span class="attribute">1 [nbench]
2 category </span>=<span class="string"> Performance cpu</span>
</code></pre><p>In this kind, the category only shows it belongs to the ‘Performance cpu’, the parser will return a dictionary, the dictionary looks like ‘{‘sincore_int’:{key1:value1, key2: value2 ….}}’, so that in yaml file, the category can be interpreted to ‘performance cpu sincore_int key1’. If the parser return a dictionary, then all values in the dictionary will use the function to do the normalisation.</p>
<p>3) Return a dictionary  (the length of category is 3)</p>
<pre><code><span class="number">1</span> [iozone]
<span class="number">2</span> category = Performance disk bandwidth  (iozone cover <span class="operator">the</span> <span class="built_in">read</span>, <span class="built_in">write</span> <span class="operator">and</span> so <span class="command"><span class="keyword">on</span> <span class="title">in</span> <span class="title">bandwidth</span>)</span>
<span class="number">3</span> scores_way =  compute_speed_score <span class="number">5</span> 
</code></pre><p>The parser will return a dictionary, it looks like {‘read’:’1234’, ‘write’: ‘780’, …}. All the value in the dictionary will be computed by the ‘compute_speed_score 5 ‘ later.</p>
<p>4) Return a number (the length of category is 4)</p>
<pre><code><span class="number">1</span> [scimark]
<span class="number">2</span> <span class="variable">category =</span> Performance cpu multicore_float scimark
<span class="number">3</span> <span class="variable">scores_way =</span>  compute_speed_score <span class="number">1</span>
</code></pre><p>If the command is execcuted successfully, the function of parser return a float number; or the 0 should be returned. </p>
<h5 id="6-_‘compute_the_score’_for_the_benchmark">6. ‘compute the score’ for the benchmark</h5>
<p>6.1 For latency, we can provide the <code>exp_score_compute</code> to compute, it has two parameter, one is base, and the other is a index.<br>It has the function of <code>score = (value/(10**base))** index</code>, the index is a negtive number.</p>
<p>6.2 For the values that is <strong>the more, the better</strong>, we provide the function of <code>compute_speed_score</code>, it has the same function of <code>score = value / (10**parameter)</code></p>
<h5 id="7-_Generate_the_yaml_file">7. Generate the yaml file</h5>
<p>The values generated by the run and parser will be stored in the yaml file, the hoatname.yaml store the original values. the hostname_score.yaml store the normalized values, the compute method is defined in the <code>scores_way =  compute_speed_score 5</code>. The hostname_score_post.yaml has got the total value from each point scores. It will be used for drawing graph.</p>

        </section>
        <hr/>
        <nav class="pagination" style="width:auto" role="pagination">
            
            <a data-pjax class="newer-posts" href="/2015/08/10/Shared-Boards-in-Open-Lab/">← Prev Post</a>
            
            <a class="share-button" data-original-title title>Share this Post</a>
            
            <a data-pjax class="older-posts" href="/2015/08/10/UEFI-and-Grub/">Next Post →</a>
            
        </nav>
        <br/>
        <br/>
        <section id="comment">
            <div id="comment-box"></div>
        </section>


    </article>
</main>


  
<footer class="site-footer">
    
    <div class="inner">
        <section class="copyright"><a href="/"></a> &copy; Open-Estuary 2014-2015</section>
        <section class="poweredby">Published with <a target="_blank" href="http://hexo.io/">Hexo   </a> and Theme by <a target="_blank" href="https://github.com/yuche/hexo-theme-kael">Kael</a></section>

		
<script type="text/javascript">
var currenthref = window.location.href;
if(currenthref == "http://open-estuary.github.io/" || currenthref == "http://localhost:4000/"){
var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1254398477'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s11.cnzz.com/z_stat.php%3Fid%3D1254398477%26online%3D1%26show%3Dline' type='text/javascript'%3E%3C/script%3E"));
}

</script>



    </div>
</footer>
</div>
</div><!-- /scroller -->

</div><!-- /pusher -->
</div><!-- /container -->
</div>

<!-- Easter eggs -->

<div class="egg animated">
    <a id="close-button" href="#">X</a>
    <div class="block">
        <div class="loading">
            <span class="ball1"></span>
            <span class="ball2"></span>
        </div>
    </div>
</div>

  
<script src="//cdn.staticfile.org/jquery/1.11.0/jquery.min.js"></script>
<script>
    if (!window.jQuery) {
        var script = document.createElement('script');
        script.src = "/js/jquery.min.js";
        document.body.appendChild(script);
    }
</script>
<script type="text/javascript" src="/js/lib.js"></script>
<script type="text/javascript" src="/js/main.js"></script>
<script type="text/javascript" src="/js/jquery.metro-btn.js"></script>






</body>
</html>
